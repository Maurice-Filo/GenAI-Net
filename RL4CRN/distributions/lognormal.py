r"""
Multivariate log-normal distribution.

This module defines `MultivariateLogNormal`, a distribution obtained by
exponentiating a multivariate normal random variable:

$$X \sim \mathcal{N}(\mu, \Sigma), \qquad Y = \exp(X),$$

where the exponential is applied element-wise. The parameters `loc` and
`covariance_matrix` therefore live in *log-space* (they parameterize $X$,
not $Y$).

Change of variables:
    The log-density is computed via the standard Jacobian correction. For
    $y \in \mathbb{R}^d_{+}$ and $x = \log y$:

    $$\log p_Y(y) = \log p_X(\log y) - \sum_{i=1}^d \log y_i
                 = \log p_X(x) - \sum_{i=1}^d x_i.$$

Moments:
    For $X \sim \mathcal{N}(\mu, \Sigma)$ and $Y = \exp(X)$:

    $$\mathbb{E}[Y_i] = \exp\left(\mu_i + \tfrac{1}{2}\Sigma_{ii}\right),$$


    $$\operatorname{Var}(Y_i)
    = \left(\exp(\Sigma_{ii}) - 1\right)\exp\left(2\mu_i + \Sigma_{ii}\right),$$


    $$\operatorname{Cov}(Y_i, Y_j)
    = \exp\left(\mu_i + \mu_j + \tfrac{1}{2}(\Sigma_{ii} + \Sigma_{jj})\right)
      \left(\exp(\Sigma_{ij}) - 1\right).$$

Entropy:
    Using the transformation $Y = \exp(X)$,


    $$H(Y) = H(X) + \sum_{i=1}^d \mu_i.$$

Support:
    `Y` is element-wise nonnegative; the distribution uses
    `constraints.independent(constraints.nonnegative, 1)` as support.
"""

import torch
from torch.distributions import Distribution, constraints, MultivariateNormal

class MultivariateLogNormal(Distribution):
    r"""Multivariate log-normal distribution implemented via a base MVN in log-space.

    Parameters are specified in log-space:
    $X \sim \mathcal{N}(\mu, \Sigma)$ with loc $\mu$
    covariance matrix $\Sigma$, and samples $Y = \exp(X)$. 

    Args:
        loc: Mean vector $\mu$ of the underlying multivariate normal in
            log-space. Shape `(..., d)`.
        covariance_matrix: Covariance matrix $\Sigma$ of the underlying
            multivariate normal in log-space. Shape `(..., d, d)`.
        validate_args: Passed to `torch.distributions.Distribution` to
            enable/disable argument validation.

    Attributes:
        mvn: The underlying `torch.distributions.MultivariateNormal`
            distribution for $X$.
    """
    # Parameters live in *log-space*: X ~ N(loc, covariance_matrix), Y = exp(X)
    arg_constraints = {
        "loc": constraints.real_vector,
        "covariance_matrix": constraints.positive_definite,
    }
    support = constraints.independent(constraints.nonnegative, 1)
    has_rsample = True

    def __init__(self, loc, covariance_matrix, validate_args=None):
        self.loc = torch.as_tensor(loc)
        self.covariance_matrix = torch.as_tensor(covariance_matrix)  # log-space Σ
        self.mvn = MultivariateNormal(self.loc, covariance_matrix=self.covariance_matrix)
        super().__init__(
            batch_shape=self.mvn.batch_shape,
            event_shape=self.mvn.event_shape,
            validate_args=validate_args,
        )

    def sample(self, sample_shape=torch.Size()):
        """Draw samples.

        Samples are generated by sampling $X \sim \mathcal{N}(\mu,\Sigma)$
        and returning $Y = \exp(X)$.

        Args:
            sample_shape: Optional leading sample shape.

        Returns:
            Samples of shape `sample_shape + batch_shape + event_shape`.
        """
        z = self.mvn.sample(sample_shape)   
        return z.exp()
    

    def log_prob(self, value):
        """Compute log-probability of a sample.

        Uses the change-of-variables identity:

        $$\log p_Y(y) = \log p_X(\log y) - \sum_i \log y_i.$$

        Args:
            value: Tensor of samples `y` with shape `(..., d)`.

        Returns:
            Tensor of log-probabilities with shape `value.shape[:-1]`.

        Raises:
            ValueError: If `validate_args=True` and `value` violates the support
                constraint (nonnegativity).
        """
        if self._validate_args:
            self._validate_sample(value)
        x = value.log()
        # log p_Y(y) = log p_X(log y) - sum_i log y_i
        return self.mvn.log_prob(x) - x.sum(dim=-1)

    @property
    def mean(self):
        r"""Mean of the distribution.

        For $Y = \exp(X)$ with $X \sim \mathcal{N}(\mu,\Sigma)$:

        $$\mathbb{E}[Y_i] = \exp\left(\mu_i + \tfrac{1}{2}\Sigma_{ii}\right).$$

        Returns:
            Tensor of shape `batch_shape + (d,)`.
        """
        μ = self.loc
        Σ = self.covariance_matrix
        diagΣ = torch.diagonal(Σ, dim1=-2, dim2=-1)
        return torch.exp(μ + 0.5 * diagΣ)

    @property
    def variance(self):
        r"""Marginal variances (diagonal of the covariance of $Y$).

    
        $$\operatorname{Var}(Y_i) = \left(\exp(\Sigma_{ii}) - 1\right) \exp\left(2\mu_i + \Sigma_{ii}\right).$$

        Returns:
            Tensor of shape `batch_shape + (d,)`.
        """
        # Var(Y_i) = (e^{Σ_ii} - 1) e^{2μ_i + Σ_ii}
        μ = self.loc
        Σ = self.covariance_matrix
        diagΣ = torch.diagonal(Σ, dim1=-2, dim2=-1)
        return (torch.exp(diagΣ) - 1.0) * torch.exp(2 * μ + diagΣ)

    @property
    def real_covariance_matrix(self):
        r"""Covariance matrix of $Y$ in real space.

        $$\operatorname{Cov}(Y_i, Y_j) = \exp\left(\mu_i + \mu_j + \tfrac{1}{2}(\Sigma_{ii} + \Sigma_{jj})\right) \left(\exp(\Sigma_{ij}) - 1\right).$$

        Returns:
            Tensor of shape `batch_shape + (d, d)`.
        """
        # Cov(Y_i, Y_j) = exp(μ_i+μ_j + 0.5(Σ_ii+Σ_jj)) * (exp(Σ_ij) - 1)
        μ = self.loc
        Σ = self.covariance_matrix
        diagΣ = torch.diagonal(Σ, dim1=-2, dim2=-1)
        pre = torch.exp(μ[..., :, None] + μ[..., None, :] +
                        0.5 * (diagΣ[..., :, None] + diagΣ[..., None, :]))
        return pre * (torch.exp(Σ) - 1.0)

    def entropy(self):
        """Differential entropy of the log-normal distribution.

        Using $Y = \exp(X)$ with $X \sim \mathcal{N}(\mu,\Sigma)$:

        $$H(Y) = H(X) + \sum_i \mu_i.$$

        Returns:
            Tensor of entropies with shape `batch_shape`.
        """
        # H(Y) = H(X) + sum_i μ_i, with X ~ N(μ, Σ)
        return self.mvn.entropy() + self.loc.sum(-1)
